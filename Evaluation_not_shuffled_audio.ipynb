{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import strftime, localtime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score,f1_score,accuracy_score, precision_score, recall_score, classification_report, roc_auc_score, \\\n",
    "    hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "#TODO: fix directories\n",
    "SOURCE_PATH = \"/srv/workspace/research/user_based_contexts_tagging/\"\n",
    "SPECTROGRAMS_PATH = \"/srv/workspace/research/user_based_contexts_tagging/dataset/\"\n",
    "OUTPUT_PATH = \"/srv/workspace/research/user_based_contexts_tagging/experiments_results/\"\n",
    "EXTRA_OUTPUTS = \"/srv/workspace/research/extra_experiment_results\"\n",
    "\n",
    "\n",
    "EXPERIMENTNAME = \"single_label_deeper_weighted_normalized\"\n",
    "INPUT_SHAPE = (646, 96, 1)\n",
    "EMBEDDINGS_DIM = 256\n",
    "#TODO: fix labels\n",
    "LABELS_LIST = ['car', 'gym', 'happy', 'night', 'relax',\n",
    "       'running', 'sad', 'summer', 'work', 'workout']\n",
    "\n",
    "global_user_embeddings = pd.read_pickle(\"/srv/workspace/research/user_based_contexts_tagging/GroundTruth/user_embeddings.pkl\")\n",
    "global_labels = pd.read_csv(\"/srv/workspace/research/user_based_contexts_tagging/GroundTruth/all_labels_clipped.csv\")\n",
    "train_partial = pd.read_csv(\"/srv/workspace/research/user_based_contexts_tagging/GroundTruth/train_single.csv\")\n",
    "POS_WEIGHTS = len(train_partial)/train_partial.sum()[2:]\n",
    "POS_WEIGHTS = [np.float32(x) for x in POS_WEIGHTS]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "\n",
    "def limit_memory_usage(gpu_memory_fraction=0.1):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = gpu_memory_fraction\n",
    "    set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "limit_memory_usage(0.3)\n",
    "def dataset_from_csv(csv_path, **kwargs):\n",
    "    \"\"\"\n",
    "        Load dataset from a csv file.\n",
    "        kwargs are forwarded to the pandas.read_csv function.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, **kwargs)\n",
    "\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                key:df[key].values\n",
    "                for key in df\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def set_tensor_shape(tensor, tensor_shape):\n",
    "        \"\"\"\n",
    "            set shape for a tensor (not in place, as opposed to tf.set_shape)\n",
    "        \"\"\"\n",
    "        tensor.set_shape(tensor_shape)\n",
    "        return tensor\n",
    "\n",
    "def check_tensor_shape(tensor_tf, target_shape):\n",
    "    \"\"\"\n",
    "        Return a Tensorflow boolean graph that indicates whether sample[features_key] has the specified target shape\n",
    "        Only check not None entries of target_shape.\n",
    "    \"\"\"\n",
    "    res = tf.constant(True)\n",
    "    for idx,target_length in enumerate(target_shape):\n",
    "        if target_length:\n",
    "            res = tf.logical_and(res, tf.equal(tf.constant(target_length), tf.shape(tensor_tf)[idx]))\n",
    "\n",
    "    return res\n",
    "def load_spectrogram(*args):\n",
    "    \"\"\"\n",
    "        loads spectrogram with error tracking.\n",
    "        args : song ID, path to dataset\n",
    "        return:\n",
    "            Features: numpy ndarray, computed features (if no error occured, otherwise: 0)\n",
    "            Error: boolean, False if no error, True if an error was raised during features computation.\n",
    "    \"\"\"\n",
    "    # TODO: edit path\n",
    "    path = SPECTROGRAMS_PATH\n",
    "    song_id, dummy_path = args\n",
    "    try:\n",
    "        # tf.logging.info(f\"Load spectrogram for {song_id}\")\n",
    "        spect = np.load(os.path.join(path, \"mels\" + str(song_id) + '.npz'))['arr_0']\n",
    "        if (spect.shape != (1, 646, 96)):\n",
    "            # print(\"\\n Error while computing features for\" +  str(song_id) + '\\n')\n",
    "            return np.float32(0.0), True\n",
    "            # spect = spect[:,215:215+646]\n",
    "        # print(spect.shape)\n",
    "        return spect, False\n",
    "    except Exception as err:\n",
    "        # print(\"\\n Error while computing features for \" + str(song_id) + '\\n')\n",
    "        return np.float32(0.0), True\n",
    "\n",
    "def load_spectrogram_tf(sample, identifier_key=\"song_id\",\n",
    "                        path=\"/my_data/MelSpectograms_top20/\", device=\"/cpu:0\",\n",
    "                        features_key=\"features\"):\n",
    "    \"\"\"\n",
    "        wrap load_spectrogram into a tensorflow function.\n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[identifier_key], tf.constant(path)]\n",
    "        res = tf.py_func(load_spectrogram,\n",
    "                         input_args,\n",
    "                         (tf.float32, tf.bool),\n",
    "                         stateful=False),\n",
    "        spectrogram, error = res[0]\n",
    "\n",
    "        res = dict(list(sample.items()) + [(features_key, spectrogram), (\"error\", error)])\n",
    "        return res\n",
    "\n",
    "\n",
    "# Dataset pipelines\n",
    "def get_embeddings_py(sample_user_id):\n",
    "    user_embeddings = global_user_embeddings[global_user_embeddings.user_id == sample_user_id]\n",
    "    samples_user_embeddings = user_embeddings.iloc[:, 1:].values.flatten()\n",
    "    samples_user_embeddings = np.asarray(samples_user_embeddings[0])\n",
    "    samples_user_embeddings = samples_user_embeddings.astype(np.float32)\n",
    "    return samples_user_embeddings\n",
    "\n",
    "\n",
    "def tf_get_embeddings_py(sample, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[\"user_id\"]]\n",
    "        user_embeddings = tf.py_func(get_embeddings_py,\n",
    "                                                        input_args,\n",
    "                                                        [tf.float32],\n",
    "                                                        stateful=False)\n",
    "        res = dict(\n",
    "            list(sample.items()) + [(\"user_embeddings\", user_embeddings)])\n",
    "        return res\n",
    "\n",
    "# Dataset pipelines\n",
    "def get_labels_py(song_id,user_id):\n",
    "    labels = global_labels[global_labels.song_id == song_id][global_labels.user_id == user_id]\n",
    "    labels = labels.iloc[:, 2:].values.flatten() # TODO: fix this shift in dataframe columns when read\n",
    "    labels = labels.astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def tf_get_labels_py(sample, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[\"song_id\"],sample[\"user_id\"]]\n",
    "        labels = tf.py_func(get_labels_py,\n",
    "                            input_args,\n",
    "                            [tf.float32],\n",
    "                            stateful=False)\n",
    "        res = dict(list(sample.items()) + [(\"binary_label\", labels)])\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_dataset(input_csv, input_shape=INPUT_SHAPE, batch_size=32, shuffle=True,\n",
    "                infinite_generator=True, random_crop=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/\"),\n",
    "                num_parallel_calls=32):\n",
    "    # build dataset from csv file\n",
    "    dataset = dataset_from_csv(input_csv)\n",
    "    # Shuffle data\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100, seed=1, reshuffle_each_iteration=True)\n",
    "\n",
    "    # compute mel spectrogram\n",
    "    dataset = dataset.map(lambda sample: load_spectrogram_tf(sample), num_parallel_calls=1)\n",
    "\n",
    "    # filter out errors\n",
    "    dataset = dataset.filter(lambda sample: tf.logical_not(sample[\"error\"]))\n",
    "\n",
    "    # map dynamic compression\n",
    "    C = 100\n",
    "    dataset = dataset.map(lambda sample: dict(sample, features=tf.log(1 + C * sample[\"features\"])),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Apply permute dimensions\n",
    "    dataset = dataset.map(lambda sample: dict(sample, features=tf.transpose(sample[\"features\"], perm=[1, 2, 0])),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Filter by shape (remove badly shaped tensors)\n",
    "    dataset = dataset.filter(lambda sample: check_tensor_shape(sample[\"features\"], input_shape))\n",
    "\n",
    "    # set features shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample,\n",
    "                                              features=set_tensor_shape(sample[\"features\"], input_shape)))\n",
    "\n",
    "    # if cache_dir:\n",
    "    #    os.makedirs(cache_dir, exist_ok=True)\n",
    "    #    dataset = dataset.cache(cache_dir)\n",
    "\n",
    "    dataset = dataset.map(lambda sample: tf_get_labels_py(sample), num_parallel_calls=1)\n",
    "\n",
    "\n",
    "    # set output shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample, binary_label=set_tensor_shape(\n",
    "        sample[\"binary_label\"], (len(LABELS_LIST)))))\n",
    "\n",
    "    # load embeddings\n",
    "    dataset = dataset.map(lambda sample: tf_get_embeddings_py(sample), num_parallel_calls=1)\n",
    "    # set weights shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample, user_embeddings=set_tensor_shape(\n",
    "        sample[\"user_embeddings\"], EMBEDDINGS_DIM)))\n",
    "\n",
    "    if infinite_generator:\n",
    "        # Repeat indefinitly\n",
    "        dataset = dataset.repeat(count=-1)\n",
    "\n",
    "    # Make batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Select only features and annotation\n",
    "    dataset = dataset.map(lambda sample: (\n",
    "    sample[\"features\"], sample[\"binary_label\"], sample[\"user_embeddings\"],sample[\"song_id\"],sample[\"user_id\"]))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset(path):\n",
    "    return get_dataset(path, shuffle=True,\n",
    "                       cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/training/\"))\n",
    "\n",
    "\n",
    "def get_validation_dataset(path):\n",
    "    return get_dataset(path, batch_size=32, shuffle=False,\n",
    "                       random_crop=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/validation/\"))\n",
    "\n",
    "\n",
    "def get_test_dataset(path):\n",
    "    return get_dataset(path, batch_size=50, shuffle=False,\n",
    "                       infinite_generator=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/test/\"))\n",
    "\n",
    "\n",
    "def load_test_set_raw(LOADING_PATH=os.path.join(SOURCE_PATH, \"GroundTruth/\"),\n",
    "                      SPECTROGRAM_PATH=SPECTROGRAMS_PATH):\n",
    "    # Loading testset groundtruth\n",
    "    test_ground_truth = pd.read_csv(os.path.join(LOADING_PATH, \"test_ground_truth_binarized.csv\"))\n",
    "    all_ground_truth = pd.read_csv(os.path.join(LOADING_PATH, \"balanced_ground_truth_hot_vector.csv\"))\n",
    "    # all_ground_truth.drop(\"playlists_count\", axis=1, inplace=True);\n",
    "    all_ground_truth = all_ground_truth[all_ground_truth.song_id.isin(test_ground_truth.song_id)]\n",
    "    all_ground_truth = all_ground_truth.set_index('song_id')\n",
    "    all_ground_truth = all_ground_truth.loc[test_ground_truth.song_id]\n",
    "    test_classes = all_ground_truth.values\n",
    "    test_classes = test_classes.astype(int)\n",
    "\n",
    "    spectrograms = np.zeros([len(test_ground_truth), 646, 96])\n",
    "    songs_ID = np.zeros([len(test_ground_truth), 1])\n",
    "    for idx, filename in enumerate(list(test_ground_truth.song_id)):\n",
    "        try:\n",
    "            spect = np.load(os.path.join(SPECTROGRAM_PATH, str(filename) + '.npz'))['arr_0']\n",
    "        except:\n",
    "            continue\n",
    "        if (spect.shape == (1, 646, 96)):\n",
    "            spectrograms[idx] = spect\n",
    "            songs_ID[idx] = filename\n",
    "\n",
    "    # Apply same transformation as trianing [ALWAYS DOUBLE CHECK TRAINING PARAMETERS]\n",
    "    C = 100\n",
    "    spectrograms = np.log(1 + C * spectrograms)\n",
    "\n",
    "    spectrograms = np.expand_dims(spectrograms, axis=3)\n",
    "    return spectrograms, test_classes\n",
    "\n",
    "\n",
    "def get_weights(shape):\n",
    "    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "    # variable_summaries(w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    # variable_summaries(b)\n",
    "    return b\n",
    "\n",
    "\n",
    "def conv_2d(x, W, name=\"\"):\n",
    "    return tf.nn.conv2d(x, W, [1, 1, 1, 1], padding=\"SAME\", name=name)\n",
    "\n",
    "\n",
    "def max_pooling(x, shape, name=\"\"):\n",
    "    return tf.nn.max_pool(x, shape, strides=[1, 2, 2, 1], padding=\"SAME\", name=name)\n",
    "\n",
    "\n",
    "def conv_layer_with_relu(input, shape, name=\"\"):\n",
    "    W = get_weights(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv_2d(input, W, name) + b)\n",
    "\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = get_weights([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def get_model(x_input,user_embeddings, current_keep_prob, train_phase):\n",
    "    # Define model architecture\n",
    "    # C4_model\n",
    "    x_norm = tf.layers.batch_normalization(x_input, training=train_phase)\n",
    "    embeds_norm = tf.layers.batch_normalization(user_embeddings, training=train_phase)\n",
    "\n",
    "\n",
    "    with tf.name_scope('CNN_1'):\n",
    "        conv1 = conv_layer_with_relu(x_norm, [3, 3, 1, 32], name=\"conv_1\")\n",
    "        max1 = max_pooling(conv1, shape=[1, 2, 2, 1], name=\"max_pool_1\")\n",
    "\n",
    "    with tf.name_scope('CNN_2'):\n",
    "        conv2 = conv_layer_with_relu(max1, [3, 3, 32, 64], name=\"conv_2\")\n",
    "        max2 = max_pooling(conv2, shape=[1, 2, 2, 1], name=\"max_pool_2\")\n",
    "\n",
    "    with tf.name_scope('CNN_3'):\n",
    "        conv3 = conv_layer_with_relu(max2, [3, 3, 64, 128], name=\"conv_3\")\n",
    "        max3 = max_pooling(conv3, shape=[1, 2, 2, 1], name=\"max_pool_3\")\n",
    "\n",
    "    with tf.name_scope('CNN_4'):\n",
    "        conv4 = conv_layer_with_relu(max3, [3, 3, 128, 256], name=\"conv_4\")\n",
    "        max4 = max_pooling(conv4, shape=[1, 2, 2, 1], name=\"max_pool_4\")\n",
    "    \"\"\"\n",
    "    with tf.name_scope('CNN_5'):\n",
    "        conv5 = conv_layer_with_relu(max4, [3, 3, 256, 256], name=\"conv_5\")\n",
    "        max5 = max_pooling(conv5, shape=[1, 2, 2, 1], name=\"max_pool_5\")\n",
    "\n",
    "    with tf.name_scope('CNN_6'):\n",
    "        conv6 = conv_layer_with_relu(max5, [3, 3, 256, 256], name=\"conv_6\")\n",
    "        max6 = max_pooling(conv6, shape=[1, 2, 2, 1], name=\"max_pool_6\")\n",
    "\n",
    "    with tf.name_scope('Fully_connected_1'):\n",
    "        flattened = tf.reshape(max6, [-1, 11 * 2 * 256])\n",
    "        fully1 = tf.nn.sigmoid(full_layer(flattened, 256))\n",
    "    \"\"\"\n",
    "    with tf.name_scope('embedding_layer_1'):\n",
    "        embeddings_1 = full_layer(embeds_norm, 128)\n",
    "        embeds_1_norm = tf.layers.batch_normalization(embeddings_1, training=train_phase)\n",
    "    \"\"\"\n",
    "    with tf.name_scope('flattened_layer_1'):\n",
    "        flattened = tf.reshape(max4, [-1, 41 * 6 * 256])\n",
    "        spect_1 = tf.nn.sigmoid(full_layer(flattened, 2048))\n",
    "        spect_2 = tf.nn.sigmoid(full_layer(spect_1, 256))\n",
    "        spect_3 = tf.nn.sigmoid(full_layer(spect_2, 128))\n",
    "    \"\"\"\n",
    "    with tf.name_scope('Fully_connected_1'):\n",
    "        flattened = tf.reshape(max4, [-1, 41 * 6 * 256])\n",
    "        flattened_norm = tf.layers.batch_normalization(flattened, training=train_phase)\n",
    "        concatenated = tf.concat([flattened_norm,embeds_1_norm],1)\n",
    "        fully1 = tf.nn.sigmoid(full_layer(concatenated, 128))\n",
    "\n",
    "\n",
    "    with tf.name_scope('Fully_connected_2'):\n",
    "        dropped = tf.nn.dropout(fully1, keep_prob=current_keep_prob)\n",
    "        logits = full_layer(dropped, len(LABELS_LIST))\n",
    "\n",
    "    output = tf.nn.softmax(logits)\n",
    "    tf.summary.histogram('outputs', output)\n",
    "    return logits, output\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(test_pred_prob, test_classes, saving_path, evaluation_file_path):\n",
    "    \"\"\"\n",
    "    Evaluates a given model using accuracy, area under curve and hamming loss\n",
    "    :param model: model to be evaluated\n",
    "    :param spectrograms: the test set spectrograms as an np.array\n",
    "    :param test_classes: the ground truth labels\n",
    "    :return: accuracy, auc_roc, hamming_error\n",
    "    \"\"\"\n",
    "    test_pred = np.round(test_pred_prob)\n",
    "    # Accuracy\n",
    "    accuracy = 100 * accuracy_score(test_classes, test_pred)\n",
    "    print(\"Exact match accuracy is: \" + str(accuracy) + \"%\")\n",
    "    # Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "    auc_roc = roc_auc_score(test_classes, test_pred_prob)\n",
    "    print(\"Macro Area Under the Curve (AUC) is: \" + str(auc_roc))\n",
    "    auc_roc_micro = roc_auc_score(test_classes, test_pred_prob, average=\"micro\")\n",
    "    print(\"Micro Area Under the Curve (AUC) is: \" + str(auc_roc_micro))\n",
    "    auc_roc_weighted = roc_auc_score(test_classes, test_pred_prob, average=\"weighted\")\n",
    "    print(\"Weighted Area Under the Curve (AUC) is: \" + str(auc_roc_weighted))\n",
    "    # Hamming loss is the fraction of labels that are incorrectly predicted.\n",
    "    hamming_error = hamming_loss(test_classes, test_pred)\n",
    "    print(\"Hamming Loss (ratio of incorrect tags) is: \" + str(hamming_error))\n",
    "    with open(evaluation_file_path, \"w\") as f:\n",
    "        f.write(\"Exact match accuracy is: \" + str(accuracy) + \"%\\n\" + \"Area Under the Curve (AUC) is: \" + str(auc_roc)\n",
    "                + \"\\nMicro AUC is:\" + str(auc_roc_micro) + \"\\nWeighted AUC is:\" + str(auc_roc_weighted)\n",
    "                + \"\\nHamming Loss (ratio of incorrect tags) is: \" + str(hamming_error))\n",
    "    print(\"saving prediction to disk\")\n",
    "    np.savetxt(os.path.join(saving_path, 'predictions.out'), test_pred_prob, delimiter=',')\n",
    "    np.savetxt(os.path.join(saving_path, 'test_ground_truth_classes.txt'), test_classes, delimiter=',')\n",
    "    return accuracy, auc_roc, hamming_error\n",
    "\n",
    "\n",
    "def plot_loss_acuracy(epoch_losses_history, epoch_accurcies_history, val_losses_history, val_accuracies_history, path):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epoch_accurcies_history)\n",
    "    plt.plot(val_accuracies_history)\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(path, \"model_accuracy.png\"))\n",
    "    plt.savefig(os.path.join(path, \"model_accuracy.pdf\"), format='pdf')\n",
    "    # plt.savefig(os.path.join(path,label + \"_model_accuracy.eps\"), format='eps', dpi=900)\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epoch_losses_history)\n",
    "    plt.plot(val_losses_history)\n",
    "    plt.title('Model loss (Cross Entropy without weighting)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(path, \"model_loss.png\"))\n",
    "    plt.savefig(os.path.join(path, \"model_loss.pdf\"), format='pdf')\n",
    "    # plt.savefig(os.path.join(path,label + \"_model_loss.eps\"), format='eps', dpi=900)\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "\n",
    "def load_predictions_groundtruth(predictions_path, groundtruth_path):\n",
    "    test_pred_prob = np.loadtxt(predictions_path, delimiter=',')\n",
    "    test_classes = np.loadtxt(groundtruth_path, delimiter=',')\n",
    "    return test_pred_prob, test_classes\n",
    "\n",
    "\n",
    "def plot_output_coocurances(model_output_rounded, output_path, LABELS_LIST):\n",
    "    # Getting coocuarances\n",
    "    test_pred_df = pd.DataFrame(model_output_rounded, columns=LABELS_LIST)\n",
    "    coocurrances = pd.DataFrame(columns=test_pred_df.columns)\n",
    "    for column in test_pred_df.columns:\n",
    "        coocurrances[column] = test_pred_df[test_pred_df[column] == 1].sum()\n",
    "    coocurrances = coocurrances.T\n",
    "    # Plotting coocurances\n",
    "    plt.figure(figsize=(30, 30));\n",
    "    sn.set(font_scale=2)  # for label size\n",
    "    cmap = 'PuRd'\n",
    "    plt.axes([.1, .1, .8, .7])\n",
    "    plt.figtext(.5, .83, 'Number of track coocurances in model output', fontsize=34, ha='center')\n",
    "    sn.heatmap(coocurrances, annot=True, annot_kws={\"size\": 24}, fmt='.0f', cmap=cmap);\n",
    "    plt.savefig(output_path + \".pdf\", format=\"pdf\")\n",
    "    plt.savefig(output_path + \".png\")\n",
    "\n",
    "\n",
    "def plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth, output_path, LABELS_LIST):\n",
    "    # Getting false negatives coocuarances\n",
    "    test_pred_df = pd.DataFrame(model_output_rounded, columns=LABELS_LIST)\n",
    "    test_classes_df = pd.DataFrame(groundtruth, columns=LABELS_LIST)\n",
    "    FN_coocurrances = pd.DataFrame(columns=test_pred_df.columns)\n",
    "    for column in test_pred_df.columns:\n",
    "        FN_coocurrances[column] = test_pred_df[[negative_prediction and positive_sample\n",
    "                                                for negative_prediction, positive_sample in\n",
    "                                                zip(test_pred_df[column] == 0, test_classes_df[column] == 1)]].sum()\n",
    "    FN_coocurrances = FN_coocurrances.T\n",
    "    # Plotting coocurances\n",
    "    plt.figure(figsize=(30, 30));\n",
    "    sn.set(font_scale=2)  # for label size\n",
    "    cmap = 'PuRd'\n",
    "    plt.axes([.1, .1, .8, .7])\n",
    "    plt.figtext(.5, .83, 'False negatives confusion matrix', fontsize=34, ha='center')\n",
    "    sn.heatmap(FN_coocurrances, annot=True, annot_kws={\"size\": 24}, fmt='.0f', cmap=cmap);\n",
    "    plt.savefig(output_path + \".pdf\", format=\"pdf\")\n",
    "    plt.savefig(output_path + \".png\")\n",
    "\n",
    "\n",
    "def plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth, output_path, LABELS_LIST):\n",
    "    # Creating a plot of true positives vs all positives\n",
    "    true_positives_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1))\n",
    "    true_positives_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "    true_positives_df.index.astype(str, copy=False)\n",
    "    true_positives_df.loc[0] = true_positives_perclass\n",
    "    percentage_of_positives_perclass = sum(groundtruth)\n",
    "    true_positives_df.loc[1] = percentage_of_positives_perclass\n",
    "    true_positives_df.index = ['True Positives', 'Positive Samples']\n",
    "    true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(groundtruth)\n",
    "    # Plot the figure\n",
    "    labels = [label + \" (\" + \"{:.1f}\".format(true_positives_ratio_perclass[idx] * 100) + \"%) \" for idx, label in\n",
    "              enumerate(LABELS_LIST)]\n",
    "    true_positives_df.columns = labels\n",
    "    true_positives_df.T.plot.bar(figsize=(32, 22), fontsize=28)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\n",
    "        \"Number of true positive per class compared to the total number of positive samples \\n Average true positive rate: \" + \"{:.2f}\".format(\n",
    "            true_positives_ratio_perclass.mean()))\n",
    "    plt.savefig(output_path + \".pdf\", format=\"pdf\")\n",
    "    plt.savefig(output_path + \".png\")\n",
    "\n",
    "\n",
    "def create_analysis_report(model_output, groundtruth, output_path, LABELS_LIST, validation_output=None,\n",
    "                           validation_groundtruth=None):\n",
    "    \"\"\"\n",
    "    Create a report of all the different evaluation metrics, including optimizing the threshold with the validation set\n",
    "    if it is passed in the parameters\n",
    "    \"\"\"\n",
    "    # Round the probabilities at 0.5\n",
    "    model_output_rounded = np.round(model_output)\n",
    "    model_output_rounded = np.clip(model_output_rounded, 0, 1)\n",
    "    # Create a dataframe where we keep all the evaluations, starting by prediction accuracy\n",
    "    accuracies_perclass = sum(model_output_rounded == groundtruth) / len(groundtruth)\n",
    "    results_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "    results_df.index.astype(str, copy=False)\n",
    "    percentage_of_positives_perclass = sum(groundtruth) / len(groundtruth)\n",
    "    results_df.loc[0] = percentage_of_positives_perclass\n",
    "    results_df.loc[1] = accuracies_perclass\n",
    "    results_df.index = ['Ratio of positive samples', 'Model accuracy']\n",
    "\n",
    "    # plot the accuracies per class\n",
    "    results_df.T.plot.bar(figsize=(22, 12), fontsize=18)\n",
    "    plt.title('Model accuracy vs the ratio of positive samples per class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_path, \"accuracies_vs_positiveRate.pdf\"), format=\"pdf\")\n",
    "    plt.savefig(os.path.join(output_path, \"accuracies_vs_positiveRate.png\"))\n",
    "\n",
    "    # Getting the true positive rate perclass\n",
    "    true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(groundtruth)\n",
    "    results_df.loc[2] = true_positives_ratio_perclass\n",
    "    # Get true negative ratio\n",
    "    true_negative_ratio_perclass = sum((model_output_rounded == groundtruth)\n",
    "                                       * (groundtruth == 0)) / (len(groundtruth) - sum(groundtruth))\n",
    "    results_df.loc[3] = true_negative_ratio_perclass\n",
    "    # compute additional metrics (AUC,f1,recall,precision)\n",
    "    auc_roc_per_label = roc_auc_score(groundtruth, model_output, average=None)\n",
    "    precision_perlabel = precision_score(groundtruth, model_output_rounded, average=None)\n",
    "    recall_perlabel = recall_score(groundtruth, model_output_rounded, average=None)\n",
    "    f1_perlabel = f1_score(groundtruth, model_output_rounded, average=None)\n",
    "    kappa_perlabel = [cohen_kappa_score(groundtruth[:, x], model_output_rounded[:, x]) for x in range(len(LABELS_LIST))]\n",
    "    results_df = results_df.append(\n",
    "        pd.DataFrame([auc_roc_per_label,recall_perlabel, precision_perlabel, f1_perlabel, kappa_perlabel], columns=LABELS_LIST))\n",
    "    results_df.index = ['Ratio of positive samples', 'Model accuracy', 'True positives ratio',\n",
    "                        'True negatives ratio', \"AUC\", \"Recall\", \"Precision\", \"f1-score\", \"Kappa score\"]\n",
    "\n",
    "    # Creating evaluation plots\n",
    "    plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth,\n",
    "                                        os.path.join(output_path, 'TruePositive_vs_allPositives'), LABELS_LIST)\n",
    "    plot_output_coocurances(model_output_rounded, os.path.join(output_path, 'output_coocurances'), LABELS_LIST)\n",
    "    plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth,\n",
    "                                           os.path.join(output_path, 'false_negative_coocurances'), LABELS_LIST)\n",
    "\n",
    "    # Adjusting threshold based on validation set\n",
    "    if (validation_groundtruth is not None and validation_output is not None):\n",
    "        np.savetxt(os.path.join(output_path, 'validation_predictions.out'), validation_output, delimiter=',')\n",
    "        np.savetxt(os.path.join(output_path, 'valid_ground_truth_classes.txt'), validation_groundtruth, delimiter=',')\n",
    "        thresholds = np.arange(0, 1, 0.01)\n",
    "        f1_array = np.zeros((len(LABELS_LIST), len(thresholds)))\n",
    "        for idx, label in enumerate(LABELS_LIST):\n",
    "            f1_array[idx, :] = [\n",
    "                f1_score(validation_groundtruth[:, idx], np.clip(np.round(validation_output[:, idx] - threshold + 0.5), 0, 1))\n",
    "                for threshold in thresholds]\n",
    "        threshold_arg = np.argmax(f1_array, axis=1)\n",
    "        threshold_per_class = thresholds[threshold_arg]\n",
    "\n",
    "        # plot the f1 score across thresholds\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        for idx, x in enumerate(LABELS_LIST):\n",
    "            plt.plot(thresholds, f1_array[idx, :], linewidth=5)\n",
    "        plt.legend(LABELS_LIST, loc='best')\n",
    "        plt.title(\"F1 Score vs different prediction threshold values for each class\")\n",
    "        plt.savefig(os.path.join(output_path, \"f1_score_vs_thresholds.pdf\"), format=\"pdf\")\n",
    "        plt.savefig(os.path.join(output_path, \"f1_score_vs_thresholds.png\"))\n",
    "\n",
    "        # Applying thresholds optimized per class\n",
    "        model_output_rounded = np.zeros_like(model_output)\n",
    "        for idx, label in enumerate(LABELS_LIST):\n",
    "            model_output_rounded[:, idx] = np.clip(np.round(model_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\n",
    "        accuracies_perclass = sum(model_output_rounded == groundtruth) / len(groundtruth)\n",
    "        # Getting the true positive rate perclass\n",
    "        true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(\n",
    "            groundtruth)\n",
    "        # Get true negative ratio\n",
    "        true_negative_ratio_perclass = sum((model_output_rounded == groundtruth)\n",
    "                                           * (groundtruth == 0)) / (len(groundtruth) - sum(groundtruth))\n",
    "        results_df = results_df.append(\n",
    "            pd.DataFrame([accuracies_perclass, true_positives_ratio_perclass,\n",
    "                          true_negative_ratio_perclass], columns=LABELS_LIST))\n",
    "        # compute additional metrics (AUC,f1,recall,precision)\n",
    "        auc_roc_per_label = roc_auc_score(groundtruth, model_output, average=None)\n",
    "        precision_perlabel = precision_score(groundtruth, model_output_rounded, average=None)\n",
    "        recall_perlabel = recall_score(groundtruth, model_output_rounded, average=None)\n",
    "        f1_perlabel = f1_score(groundtruth, model_output_rounded, average=None)\n",
    "        kappa_perlabel = [cohen_kappa_score(groundtruth[:, x], model_output_rounded[:, x]) for x in\n",
    "                          range(len(LABELS_LIST))]\n",
    "        results_df = results_df.append(\n",
    "            pd.DataFrame([auc_roc_per_label, precision_perlabel, recall_perlabel, f1_perlabel,kappa_perlabel],\n",
    "                         columns=LABELS_LIST))\n",
    "        results_df.index = ['Ratio of positive samples', 'Model accuracy', 'True positives ratio',\n",
    "                            'True negatives ratio', \"AUC\", \"Precision\", \"Recall\", \"f1-score\",  \"Kappa score\",\n",
    "                            'Optimized model accuracy', 'Optimized true positives ratio',\n",
    "                            'Optimized true negatives ratio', \"Optimized AUC\",\n",
    "                            \"Optimized precision\", \"Optimized recall\", \"Optimized f1-score\",  \"Optimized Kappa score\"]\n",
    "\n",
    "        # Creating evaluation plots\n",
    "        plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth,\n",
    "                                            os.path.join(output_path, 'TruePositive_vs_allPositives[optimized]'),\n",
    "                                            LABELS_LIST)\n",
    "        plot_output_coocurances(model_output_rounded, os.path.join(output_path, 'output_coocurances[optimized]'),\n",
    "                                LABELS_LIST)\n",
    "        plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth,\n",
    "                                               os.path.join(output_path, 'false_negative_coocurances[optimized]'),\n",
    "                                               LABELS_LIST)\n",
    "    results_df['average'] = results_df.mean(numeric_only=True, axis=1)\n",
    "    results_df.T.to_csv(os.path.join(output_path, \"results_report.csv\"), float_format=\"%.2f\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Experiment: single_label_deeper_weighted_normalized\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-3ae30bf83c04>:13: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "INFO:tensorflow:Restoring parameters from /srv/workspace/research/extra_experiment_results/single_label_deeper_weighted_normalized/2020-05-06_21-35-34/best_validation.ckpt\n",
      "Model with best validation restored before testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:153: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Experiment: \" + EXPERIMENTNAME + \"\\n\\n\\n\")\n",
    "# Loading datasets\n",
    "# TODO: fix directories\n",
    "\n",
    "# Setting up model\n",
    "y = tf.placeholder(tf.float32, [None, len(LABELS_LIST)], name=\"true_labels\")\n",
    "x_input = tf.placeholder(tf.float32, [None, 646, 96, 1], name=\"input\")\n",
    "embeddings_input = tf.placeholder(tf.float32, [None, EMBEDDINGS_DIM], name=\"input_embeddings\")\n",
    "current_keep_prob = tf.placeholder(tf.float32, name=\"dropout_rate\")\n",
    "weights = tf.constant(POS_WEIGHTS)\n",
    "train_phase = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "logits, model_output = get_model(x_input, embeddings_input, current_keep_prob, train_phase)\n",
    "one_hot = tf.one_hot(tf.argmax(model_output, dimension = 1), depth = len(LABELS_LIST))\n",
    "\n",
    "# Defining loss and metrics\n",
    "#loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "'''\n",
    "These following lines are needed for batch normalization to work properly\n",
    "check https://timodenk.com/blog/tensorflow-batch-normalization/\n",
    "'''\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "# Setting up saving directory\n",
    "experiment_name = strftime(\"%Y-%m-%d_%H-%M-%S\", localtime())\n",
    "exp_dir = os.path.join(OUTPUT_PATH, EXPERIMENTNAME, experiment_name)\n",
    "extra_exp_dir =  os.path.join(EXTRA_OUTPUTS, EXPERIMENTNAME, \"2020-05-06_21-35-34/\")\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Write summaries to LOG_DIR -- used by TensorBoard\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Loading model with best validation\n",
    "    saver.restore(sess, os.path.join(extra_exp_dir, \"best_validation.ckpt\"))\n",
    "    print(\"Model with best validation restored before testing.\")\n",
    "\n",
    "    test_labels = pd.read_csv(os.path.join(SOURCE_PATH, \"GroundTruth/test_single.csv\"))\n",
    "    test_dataset = get_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/test_single.csv\"),shuffle = False)\n",
    "    test_classes = np.zeros_like(test_labels.iloc[:, 2:].values, dtype=float)\n",
    "    # test_images, test_classes = load_test_set_raw(test_split)\n",
    "\n",
    "    TEST_NUM_STEPS = int(np.floor((len(test_classes) / 32)))\n",
    "    # split_size = int(len(test_classes) / TEST_NUM_STEPS)\n",
    "    test_pred_prob = np.zeros_like(test_classes, dtype=float)\n",
    "    test_one_hot = np.zeros_like(test_classes, dtype=float)\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    test_next_element = test_iterator.get_next()\n",
    "    test_song_ids = np.zeros([test_classes.shape[0],1])\n",
    "    test_user_ids = np.zeros([test_classes.shape[0],1])\n",
    "\n",
    "    for test_batch_counter in range(TEST_NUM_STEPS):\n",
    "        start_idx = (test_batch_counter * BATCH_SIZE)\n",
    "        end_idx = (test_batch_counter * BATCH_SIZE) + BATCH_SIZE\n",
    "        test_batch = sess.run(test_next_element)\n",
    "        test_batch_images = test_batch[0]\n",
    "        test_batch_labels = np.squeeze(test_batch[1])\n",
    "        test_embeddings = np.squeeze(test_batch[2])\n",
    "        test_song_ids[start_idx:end_idx] = test_batch[3].reshape([-1, 1])\n",
    "        test_user_ids[start_idx:end_idx] = test_batch[4].reshape([-1, 1])\n",
    "        test_classes[start_idx:end_idx, :] = test_batch_labels\n",
    "\n",
    "        test_pred_prob[start_idx:end_idx, :],test_one_hot[start_idx:end_idx, :] = sess.run([model_output, one_hot],\n",
    "                                                        feed_dict={x_input: test_batch_images,\n",
    "                                                                   embeddings_input: test_embeddings,\n",
    "                                                                   current_keep_prob: 1.0,\n",
    "                                                                   train_phase: False})\n",
    "\n",
    "\n",
    "    np.savetxt(os.path.join(exp_dir, 'tracks_ids.txt'), test_song_ids, delimiter=',')\n",
    "    np.savetxt(os.path.join(exp_dir, 'user_ids.txt'), test_user_ids, delimiter=',')\n",
    "    np.savetxt(os.path.join(exp_dir, 'test_output_one_hot'), test_one_hot, delimiter=',')\n",
    "    accuracy_out, auc_roc, hamming_error = evaluate_model(test_pred_prob, test_classes,\n",
    "                                                          saving_path=exp_dir,\n",
    "                                                          evaluation_file_path= \\\n",
    "                                                              os.path.join(exp_dir, \"evaluation_results.txt\"))\n",
    "    results = create_analysis_report(test_pred_prob, test_classes, exp_dir, LABELS_LIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-320949a8ceb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save as dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_ground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test_active_clipped.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_groundtruth_from_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test_ground_truth_classes.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muser_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/user_ids.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrack_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/tracks_ids.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# save as dataframe \n",
    "import pandas as pd\n",
    "test_ground_truth = pd.read_csv(\"/content/test_single.csv\")\n",
    "test_groundtruth_from_model = np.loadtxt(exp_dir + \"test_ground_truth_classes.txt\",delimiter=',')\n",
    "user_ids = np.loadtxt(exp_dir + \"user_ids.txt\",delimiter=',')\n",
    "track_ids = np.loadtxt(\"/content/tracks_ids.txt\",delimiter=',')\n",
    "test_output = np.loadtxt(\"/content/predictions.out\",delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d326f3e6f685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_ground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mour_ground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msong_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mour_ground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mour_ground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_groundtruth_from_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "our_ground_truth = test_ground_truth.copy()\n",
    "our_ground_truth.song_id = track_ids\n",
    "our_ground_truth.user_id = user_ids\n",
    "our_ground_truth.iloc[:,2:] = test_groundtruth_from_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ecb3f56310e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mour_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msong_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mour_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mour_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "our_predictions = test_ground_truth.copy()\n",
    "our_predictions.song_id = track_ids\n",
    "our_predictions.user_id = user_ids\n",
    "our_predictions.iloc[:,2:] = test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
