{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import strftime, localtime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, roc_auc_score, \\\n",
    "    hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"\n",
    "\n",
    "#TODO: fix directories\n",
    "SOURCE_PATH = \"/srv/workspace/research/user_based_contexts_tagging/\"\n",
    "SPECTROGRAMS_PATH = \"/srv/workspace/research/user_based_contexts_tagging/dataset/\"\n",
    "OUTPUT_PATH = \"/srv/workspace/research/user_based_contexts_tagging/experiments_results/\"\n",
    "\n",
    "EXPERIMENTNAME = \"user_embeddings_initial\"\n",
    "INPUT_SHAPE = (646, 96, 1)\n",
    "EMBEDDINGS_DIM = 256\n",
    "#TODO: fix labels\n",
    "LABELS_LIST = ['car', 'chill', 'club', 'dance', 'gym', 'happy',\n",
    "       'night', 'party', 'relax', 'running', 'sad', 'sleep', 'summer', 'work','workout']\n",
    "\n",
    "global_user_embeddings = pd.read_pickle(\"/srv/workspace/research/user_based_contexts_tagging/GroundTruth/user_embeddings.pkl\")\n",
    "global_labels = pd.read_csv(\"/srv/workspace/research/user_based_contexts_tagging/GroundTruth/all_labels_binarized.csv\")\n",
    "train_partial = pd.read_csv(\"/srv/workspace/research/user_based_contexts_tagging/GroundTruth/train_partial.csv\")\n",
    "POS_WEIGHTS = len(train_partial)/train_partial.sum()[2:]\n",
    "POS_WEIGHTS = np.clip(POS_WEIGHTS, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "def limit_memory_usage(gpu_memory_fraction=0.1):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = gpu_memory_fraction\n",
    "    set_session(tf.Session(config=config))\n",
    "    \n",
    "limit_memory_usage(0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_csv(csv_path, **kwargs):\n",
    "    \"\"\"\n",
    "        Load dataset from a csv file.\n",
    "        kwargs are forwarded to the pandas.read_csv function.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, **kwargs)\n",
    "\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            {\n",
    "                key:df[key].values\n",
    "                for key in df\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def set_tensor_shape(tensor, tensor_shape):\n",
    "        \"\"\"\n",
    "            set shape for a tensor (not in place, as opposed to tf.set_shape)\n",
    "        \"\"\"\n",
    "        tensor.set_shape(tensor_shape)\n",
    "        return tensor\n",
    "\n",
    "def check_tensor_shape(tensor_tf, target_shape):\n",
    "    \"\"\"\n",
    "        Return a Tensorflow boolean graph that indicates whether sample[features_key] has the specified target shape\n",
    "        Only check not None entries of target_shape.\n",
    "    \"\"\"\n",
    "    res = tf.constant(True)\n",
    "    for idx,target_length in enumerate(target_shape):\n",
    "        if target_length:\n",
    "            res = tf.logical_and(res, tf.equal(tf.constant(target_length), tf.shape(tensor_tf)[idx]))\n",
    "\n",
    "    return res\n",
    "def load_spectrogram(*args):\n",
    "    \"\"\"\n",
    "        loads spectrogram with error tracking.\n",
    "        args : song ID, path to dataset\n",
    "        return:\n",
    "            Features: numpy ndarray, computed features (if no error occured, otherwise: 0)\n",
    "            Error: boolean, False if no error, True if an error was raised during features computation.\n",
    "    \"\"\"\n",
    "    # TODO: edit path\n",
    "    path = SPECTROGRAMS_PATH\n",
    "    song_id, dummy_path = args\n",
    "    try:\n",
    "        # tf.logging.info(f\"Load spectrogram for {song_id}\")\n",
    "        spect = np.load(os.path.join(path, \"mels\" + str(song_id) + '.npz'))['arr_0']\n",
    "        if (spect.shape != (1, 646, 96)):\n",
    "            # print(\"\\n Error while computing features for\" +  str(song_id) + '\\n')\n",
    "            return np.float32(0.0), True\n",
    "            # spect = spect[:,215:215+646]\n",
    "        # print(spect.shape)\n",
    "        return spect, False\n",
    "    except Exception as err:\n",
    "        # print(\"\\n Error while computing features for \" + str(song_id) + '\\n')\n",
    "        return np.float32(0.0), True\n",
    "\n",
    "def load_spectrogram_tf(sample, identifier_key=\"song_id\",\n",
    "                        path=\"/my_data/MelSpectograms_top20/\", device=\"/cpu:0\",\n",
    "                        features_key=\"features\"):\n",
    "    \"\"\"\n",
    "        wrap load_spectrogram into a tensorflow function.\n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[identifier_key], tf.constant(path)]\n",
    "        res = tf.py_func(load_spectrogram,\n",
    "                         input_args,\n",
    "                         (tf.float32, tf.bool),\n",
    "                         stateful=False),\n",
    "        spectrogram, error = res[0]\n",
    "\n",
    "        res = dict(list(sample.items()) + [(features_key, spectrogram), (\"error\", error)])\n",
    "        return res\n",
    "\n",
    "\n",
    "# Dataset pipelines\n",
    "def get_embeddings_py(sample_user_id):\n",
    "    user_embeddings = global_user_embeddings[global_user_embeddings.user_id == sample_user_id]\n",
    "    samples_user_embeddings = user_embeddings.iloc[:, 1:].values.flatten()\n",
    "    samples_user_embeddings = np.asarray(samples_user_embeddings[0])\n",
    "    samples_user_embeddings = samples_user_embeddings.astype(np.float32)\n",
    "    return samples_user_embeddings\n",
    "\n",
    "\n",
    "def tf_get_embeddings_py(sample, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[\"user_id\"]]\n",
    "        user_embeddings = tf.py_func(get_embeddings_py,\n",
    "                                                        input_args,\n",
    "                                                        [tf.float32],\n",
    "                                                        stateful=False)\n",
    "        res = dict(\n",
    "            list(sample.items()) + [(\"user_embeddings\", user_embeddings)])\n",
    "        return res\n",
    "\n",
    "# Dataset pipelines\n",
    "def get_labels_py(song_id,user_id):\n",
    "    labels = global_labels[global_labels.song_id == song_id][global_labels.user_id == user_id]\n",
    "    labels = labels.iloc[:, 2:].values.flatten() # TODO: fix this shift in dataframe columns when read\n",
    "    labels = labels.astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def tf_get_labels_py(sample, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        input_args = [sample[\"song_id\"],sample[\"user_id\"]]\n",
    "        labels = tf.py_func(get_labels_py,\n",
    "                            input_args,\n",
    "                            [tf.float32],\n",
    "                            stateful=False)\n",
    "        res = dict(list(sample.items()) + [(\"binary_label\", labels)])\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_dataset(input_csv, input_shape=INPUT_SHAPE, batch_size=32, shuffle=True,\n",
    "                infinite_generator=True, random_crop=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/\"),\n",
    "                num_parallel_calls=32):\n",
    "    # build dataset from csv file\n",
    "    dataset = dataset_from_csv(input_csv)\n",
    "    # Shuffle data\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100, seed=1, reshuffle_each_iteration=True)\n",
    "\n",
    "    # compute mel spectrogram\n",
    "    dataset = dataset.map(lambda sample: load_spectrogram_tf(sample), num_parallel_calls=1)\n",
    "\n",
    "    # filter out errors\n",
    "    dataset = dataset.filter(lambda sample: tf.logical_not(sample[\"error\"]))\n",
    "\n",
    "    # map dynamic compression\n",
    "    C = 100\n",
    "    dataset = dataset.map(lambda sample: dict(sample, features=tf.log(1 + C * sample[\"features\"])),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Apply permute dimensions\n",
    "    dataset = dataset.map(lambda sample: dict(sample, features=tf.transpose(sample[\"features\"], perm=[1, 2, 0])),\n",
    "                          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # Filter by shape (remove badly shaped tensors)\n",
    "    dataset = dataset.filter(lambda sample: check_tensor_shape(sample[\"features\"], input_shape))\n",
    "\n",
    "    # set features shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample,\n",
    "                                              features=set_tensor_shape(sample[\"features\"], input_shape)))\n",
    "\n",
    "    # if cache_dir:\n",
    "    #    os.makedirs(cache_dir, exist_ok=True)\n",
    "    #    dataset = dataset.cache(cache_dir)\n",
    "\n",
    "    dataset = dataset.map(lambda sample: tf_get_labels_py(sample), num_parallel_calls=1)\n",
    "\n",
    "\n",
    "    # set output shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample, binary_label=set_tensor_shape(\n",
    "        sample[\"binary_label\"], (len(LABELS_LIST)))))\n",
    "\n",
    "    # load embeddings\n",
    "    dataset = dataset.map(lambda sample: tf_get_embeddings_py(sample), num_parallel_calls=1)\n",
    "    # set weights shape\n",
    "    dataset = dataset.map(lambda sample: dict(sample, user_embeddings=set_tensor_shape(\n",
    "        sample[\"user_embeddings\"], EMBEDDINGS_DIM)))\n",
    "\n",
    "    if infinite_generator:\n",
    "        # Repeat indefinitly\n",
    "        dataset = dataset.repeat(count=-1)\n",
    "\n",
    "    # Make batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Select only features and annotation\n",
    "    dataset = dataset.map(lambda sample: (\n",
    "    sample[\"features\"], sample[\"binary_label\"], sample[\"user_embeddings\"]))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset(path):\n",
    "    return get_dataset(path, shuffle=True,\n",
    "                       cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/training/\"))\n",
    "\n",
    "\n",
    "def get_validation_dataset(path):\n",
    "    return get_dataset(path, batch_size=32, shuffle=False,\n",
    "                       random_crop=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/validation/\"))\n",
    "\n",
    "\n",
    "def get_test_dataset(path):\n",
    "    return get_dataset(path, batch_size=50, shuffle=False,\n",
    "                       infinite_generator=False, cache_dir=os.path.join(OUTPUT_PATH, \"tmp/tf_cache/test/\"))\n",
    "\n",
    "\n",
    "def load_test_set_raw(LOADING_PATH=os.path.join(SOURCE_PATH, \"GroundTruth/\"),\n",
    "                      SPECTROGRAM_PATH=SPECTROGRAMS_PATH):\n",
    "    # Loading testset groundtruth\n",
    "    test_ground_truth = pd.read_csv(os.path.join(LOADING_PATH, \"test_ground_truth_binarized.csv\"))\n",
    "    all_ground_truth = pd.read_csv(os.path.join(LOADING_PATH, \"balanced_ground_truth_hot_vector.csv\"))\n",
    "    # all_ground_truth.drop(\"playlists_count\", axis=1, inplace=True);\n",
    "    all_ground_truth = all_ground_truth[all_ground_truth.song_id.isin(test_ground_truth.song_id)]\n",
    "    all_ground_truth = all_ground_truth.set_index('song_id')\n",
    "    all_ground_truth = all_ground_truth.loc[test_ground_truth.song_id]\n",
    "    test_classes = all_ground_truth.values\n",
    "    test_classes = test_classes.astype(int)\n",
    "\n",
    "    spectrograms = np.zeros([len(test_ground_truth), 646, 96])\n",
    "    songs_ID = np.zeros([len(test_ground_truth), 1])\n",
    "    for idx, filename in enumerate(list(test_ground_truth.song_id)):\n",
    "        try:\n",
    "            spect = np.load(os.path.join(SPECTROGRAM_PATH, str(filename) + '.npz'))['arr_0']\n",
    "        except:\n",
    "            continue\n",
    "        if (spect.shape == (1, 646, 96)):\n",
    "            spectrograms[idx] = spect\n",
    "            songs_ID[idx] = filename\n",
    "\n",
    "    # Apply same transformation as trianing [ALWAYS DOUBLE CHECK TRAINING PARAMETERS]\n",
    "    C = 100\n",
    "    spectrograms = np.log(1 + C * spectrograms)\n",
    "\n",
    "    spectrograms = np.expand_dims(spectrograms, axis=3)\n",
    "    return spectrograms, test_classes\n",
    "\n",
    "\n",
    "def get_weights(shape):\n",
    "    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "    # variable_summaries(w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    # variable_summaries(b)\n",
    "    return b\n",
    "\n",
    "\n",
    "def conv_2d(x, W, name=\"\"):\n",
    "    return tf.nn.conv2d(x, W, [1, 1, 1, 1], padding=\"SAME\", name=name)\n",
    "\n",
    "\n",
    "def max_pooling(x, shape, name=\"\"):\n",
    "    return tf.nn.max_pool(x, shape, strides=[1, 2, 2, 1], padding=\"SAME\", name=name)\n",
    "\n",
    "\n",
    "def conv_layer_with_relu(input, shape, name=\"\"):\n",
    "    W = get_weights(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv_2d(input, W, name) + b)\n",
    "\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = get_weights([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "\n",
    "def get_model(x_input,user_embeddings, current_keep_prob, train_phase):\n",
    "    # Define model architecture\n",
    "    # C4_model\n",
    "    x_norm = tf.layers.batch_normalization(x_input, training=train_phase)\n",
    "\n",
    "    with tf.name_scope('CNN_1'):\n",
    "        conv1 = conv_layer_with_relu(x_norm, [3, 3, 1, 32], name=\"conv_1\")\n",
    "        max1 = max_pooling(conv1, shape=[1, 2, 2, 1], name=\"max_pool_1\")\n",
    "\n",
    "    with tf.name_scope('CNN_2'):\n",
    "        conv2 = conv_layer_with_relu(max1, [3, 3, 32, 64], name=\"conv_2\")\n",
    "        max2 = max_pooling(conv2, shape=[1, 2, 2, 1], name=\"max_pool_2\")\n",
    "\n",
    "    with tf.name_scope('CNN_3'):\n",
    "        conv3 = conv_layer_with_relu(max2, [3, 3, 64, 128], name=\"conv_3\")\n",
    "        max3 = max_pooling(conv3, shape=[1, 2, 2, 1], name=\"max_pool_3\")\n",
    "\n",
    "    with tf.name_scope('CNN_4'):\n",
    "        conv4 = conv_layer_with_relu(max3, [3, 3, 128, 256], name=\"conv_4\")\n",
    "        max4 = max_pooling(conv4, shape=[1, 2, 2, 1], name=\"max_pool_4\")\n",
    "    \"\"\"\n",
    "    with tf.name_scope('CNN_5'):\n",
    "        conv5 = conv_layer_with_relu(max4, [3, 3, 256, 256], name=\"conv_5\")\n",
    "        max5 = max_pooling(conv5, shape=[1, 2, 2, 1], name=\"max_pool_5\")\n",
    "\n",
    "    with tf.name_scope('CNN_6'):\n",
    "        conv6 = conv_layer_with_relu(max5, [3, 3, 256, 256], name=\"conv_6\")\n",
    "        max6 = max_pooling(conv6, shape=[1, 2, 2, 1], name=\"max_pool_6\")\n",
    "\n",
    "    with tf.name_scope('Fully_connected_1'):\n",
    "        flattened = tf.reshape(max6, [-1, 11 * 2 * 256])\n",
    "        fully1 = tf.nn.sigmoid(full_layer(flattened, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope('Fully_connected_1'):\n",
    "        flattened = tf.reshape(max4, [-1, 41 * 6 * 256])\n",
    "        concatenated = tf.concat([flattened,user_embeddings],1)\n",
    "        fully1 = tf.nn.sigmoid(full_layer(concatenated, 256))\n",
    "\n",
    "\n",
    "    with tf.name_scope('Fully_connected_2'):\n",
    "        dropped = tf.nn.dropout(fully1, keep_prob=current_keep_prob)\n",
    "        logits = full_layer(dropped, len(LABELS_LIST))\n",
    "\n",
    "    output = tf.nn.sigmoid(logits)\n",
    "    tf.summary.histogram('outputs', output)\n",
    "    return logits, output\n",
    "\n",
    "\n",
    "def evaluate_model(test_pred_prob, test_classes, saving_path, evaluation_file_path):\n",
    "    \"\"\"\n",
    "    Evaluates a given model using accuracy, area under curve and hamming loss\n",
    "    :param model: model to be evaluated\n",
    "    :param spectrograms: the test set spectrograms as an np.array\n",
    "    :param test_classes: the ground truth labels\n",
    "    :return: accuracy, auc_roc, hamming_error\n",
    "    \"\"\"\n",
    "    test_pred = np.round(test_pred_prob)\n",
    "    # Accuracy\n",
    "    accuracy = 100 * accuracy_score(test_classes, test_pred)\n",
    "    print(\"Exact match accuracy is: \" + str(accuracy) + \"%\")\n",
    "    # Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "    auc_roc = roc_auc_score(test_classes, test_pred_prob)\n",
    "    print(\"Macro Area Under the Curve (AUC) is: \" + str(auc_roc))\n",
    "    auc_roc_micro = roc_auc_score(test_classes, test_pred_prob, average=\"micro\")\n",
    "    print(\"Micro Area Under the Curve (AUC) is: \" + str(auc_roc_micro))\n",
    "    auc_roc_weighted = roc_auc_score(test_classes, test_pred_prob, average=\"weighted\")\n",
    "    print(\"Weighted Area Under the Curve (AUC) is: \" + str(auc_roc_weighted))\n",
    "    # Hamming loss is the fraction of labels that are incorrectly predicted.\n",
    "    hamming_error = hamming_loss(test_classes, test_pred)\n",
    "    print(\"Hamming Loss (ratio of incorrect tags) is: \" + str(hamming_error))\n",
    "    with open(evaluation_file_path, \"w\") as f:\n",
    "        f.write(\"Exact match accuracy is: \" + str(accuracy) + \"%\\n\" + \"Area Under the Curve (AUC) is: \" + str(auc_roc)\n",
    "                + \"\\nMicro AUC is:\" + str(auc_roc_micro) + \"\\nWeighted AUC is:\" + str(auc_roc_weighted)\n",
    "                + \"\\nHamming Loss (ratio of incorrect tags) is: \" + str(hamming_error))\n",
    "    print(\"saving prediction to disk\")\n",
    "    np.savetxt(os.path.join(saving_path, 'predictions.out'), test_pred_prob, delimiter=',')\n",
    "    np.savetxt(os.path.join(saving_path, 'test_ground_truth_classes.txt'), test_classes, delimiter=',')\n",
    "    return accuracy, auc_roc, hamming_error\n",
    "\n",
    "\n",
    "def plot_loss_acuracy(epoch_losses_history, epoch_accurcies_history, val_losses_history, val_accuracies_history, path):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epoch_accurcies_history)\n",
    "    plt.plot(val_accuracies_history)\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(path, \"model_accuracy.png\"))\n",
    "    plt.savefig(os.path.join(path, \"model_accuracy.pdf\"), format='pdf')\n",
    "    # plt.savefig(os.path.join(path,label + \"_model_accuracy.eps\"), format='eps', dpi=900)\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epoch_losses_history)\n",
    "    plt.plot(val_losses_history)\n",
    "    plt.title('Model loss (Cross Entropy without weighting)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(path, \"model_loss.png\"))\n",
    "    plt.savefig(os.path.join(path, \"model_loss.pdf\"), format='pdf')\n",
    "    # plt.savefig(os.path.join(path,label + \"_model_loss.eps\"), format='eps', dpi=900)\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "\n",
    "def load_predictions_groundtruth(predictions_path, groundtruth_path):\n",
    "    test_pred_prob = np.loadtxt(predictions_path, delimiter=',')\n",
    "    test_classes = np.loadtxt(groundtruth_path, delimiter=',')\n",
    "    return test_pred_prob, test_classes\n",
    "\n",
    "\n",
    "def plot_output_coocurances(model_output_rounded, output_path, LABELS_LIST):\n",
    "    # Getting coocuarances\n",
    "    test_pred_df = pd.DataFrame(model_output_rounded, columns=LABELS_LIST)\n",
    "    coocurrances = pd.DataFrame(columns=test_pred_df.columns)\n",
    "    for column in test_pred_df.columns:\n",
    "        coocurrances[column] = test_pred_df[test_pred_df[column] == 1].sum()\n",
    "    coocurrances = coocurrances.T\n",
    "    # Plotting coocurances\n",
    "    plt.figure(figsize=(30, 30));\n",
    "    sn.set(font_scale=2)  # for label size\n",
    "    cmap = 'PuRd'\n",
    "    plt.axes([.1, .1, .8, .7])\n",
    "    plt.figtext(.5, .83, 'Number of track coocurances in model output', fontsize=34, ha='center')\n",
    "    sn.heatmap(coocurrances, annot=True, annot_kws={\"size\": 24}, fmt='.0f', cmap=cmap);\n",
    "    plt.savefig(output_path + \".pdf\", format=\"pdf\")\n",
    "    plt.savefig(output_path + \".png\")\n",
    "\n",
    "\n",
    "def plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth, output_path, LABELS_LIST):\n",
    "    # Getting false negatives coocuarances\n",
    "    test_pred_df = pd.DataFrame(model_output_rounded, columns=LABELS_LIST)\n",
    "    test_classes_df = pd.DataFrame(groundtruth, columns=LABELS_LIST)\n",
    "    FN_coocurrances = pd.DataFrame(columns=test_pred_df.columns)\n",
    "    for column in test_pred_df.columns:\n",
    "        FN_coocurrances[column] = test_pred_df[[negative_prediction and positive_sample\n",
    "                                                for negative_prediction, positive_sample in\n",
    "                                                zip(test_pred_df[column] == 0, test_classes_df[column] == 1)]].sum()\n",
    "    FN_coocurrances = FN_coocurrances.T\n",
    "    # Plotting coocurances\n",
    "    plt.figure(figsize=(30, 30));\n",
    "    sn.set(font_scale=2)  # for label size\n",
    "    cmap = 'PuRd'\n",
    "    plt.axes([.1, .1, .8, .7])\n",
    "    plt.figtext(.5, .83, 'False negatives confusion matrix', fontsize=34, ha='center')\n",
    "    sn.heatmap(FN_coocurrances, annot=True, annot_kws={\"size\": 24}, fmt='.0f', cmap=cmap);\n",
    "    plt.savefig(output_path + \".pdf\", format=\"pdf\")\n",
    "    plt.savefig(output_path + \".png\")\n",
    "\n",
    "\n",
    "def plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth, output_path, LABELS_LIST):\n",
    "    # Creating a plot of true positives vs all positives\n",
    "    true_positives_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1))\n",
    "    true_positives_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "    true_positives_df.index.astype(str, copy=False)\n",
    "    true_positives_df.loc[0] = true_positives_perclass\n",
    "    percentage_of_positives_perclass = sum(groundtruth)\n",
    "    true_positives_df.loc[1] = percentage_of_positives_perclass\n",
    "    true_positives_df.index = ['True Positives', 'Positive Samples']\n",
    "    true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(groundtruth)\n",
    "    # Plot the figure\n",
    "    labels = [label + \" (\" + \"{:.1f}\".format(true_positives_ratio_perclass[idx] * 100) + \"%) \" for idx, label in\n",
    "              enumerate(LABELS_LIST)]\n",
    "    true_positives_df.columns = labels\n",
    "    true_positives_df.T.plot.bar(figsize=(32, 22), fontsize=28)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\n",
    "        \"Number of true positive per class compared to the total number of positive samples \\n Average true positive rate: \" + \"{:.2f}\".format(\n",
    "            true_positives_ratio_perclass.mean()))\n",
    "    plt.savefig(output_path + \".pdf\", format=\"pdf\")\n",
    "    plt.savefig(output_path + \".png\")\n",
    "\n",
    "\n",
    "def create_analysis_report(model_output, groundtruth, output_path, LABELS_LIST, validation_output=None,\n",
    "                           validation_groundtruth=None):\n",
    "    \"\"\"\n",
    "    Create a report of all the different evaluation metrics, including optimizing the threshold with the validation set\n",
    "    if it is passed in the parameters\n",
    "    \"\"\"\n",
    "    # Round the probabilities at 0.5\n",
    "    model_output_rounded = np.round(model_output)\n",
    "    model_output_rounded = np.clip(model_output_rounded, 0, 1)\n",
    "    # Create a dataframe where we keep all the evaluations, starting by prediction accuracy\n",
    "    accuracies_perclass = sum(model_output_rounded == groundtruth) / len(groundtruth)\n",
    "    results_df = pd.DataFrame(columns=LABELS_LIST)\n",
    "    results_df.index.astype(str, copy=False)\n",
    "    percentage_of_positives_perclass = sum(groundtruth) / len(groundtruth)\n",
    "    results_df.loc[0] = percentage_of_positives_perclass\n",
    "    results_df.loc[1] = accuracies_perclass\n",
    "    results_df.index = ['Ratio of positive samples', 'Model accuracy']\n",
    "\n",
    "    # plot the accuracies per class\n",
    "    results_df.T.plot.bar(figsize=(22, 12), fontsize=18)\n",
    "    plt.title('Model accuracy vs the ratio of positive samples per class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_path, \"accuracies_vs_positiveRate.pdf\"), format=\"pdf\")\n",
    "    plt.savefig(os.path.join(output_path, \"accuracies_vs_positiveRate.png\"))\n",
    "\n",
    "    # Getting the true positive rate perclass\n",
    "    true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(groundtruth)\n",
    "    results_df.loc[2] = true_positives_ratio_perclass\n",
    "    # Get true negative ratio\n",
    "    true_negative_ratio_perclass = sum((model_output_rounded == groundtruth)\n",
    "                                       * (groundtruth == 0)) / (len(groundtruth) - sum(groundtruth))\n",
    "    results_df.loc[3] = true_negative_ratio_perclass\n",
    "    # compute additional metrics (AUC,f1,recall,precision)\n",
    "    auc_roc_per_label = roc_auc_score(groundtruth, model_output, average=None)\n",
    "    precision_perlabel = precision_score(groundtruth, model_output_rounded, average=None)\n",
    "    recall_perlabel = recall_score(groundtruth, model_output_rounded, average=None)\n",
    "    f1_perlabel = f1_score(groundtruth, model_output_rounded, average=None)\n",
    "    kappa_perlabel = [cohen_kappa_score(groundtruth[:, x], model_output_rounded[:, x]) for x in range(len(LABELS_LIST))]\n",
    "    results_df = results_df.append(\n",
    "        pd.DataFrame([auc_roc_per_label,recall_perlabel, precision_perlabel, f1_perlabel, kappa_perlabel], columns=LABELS_LIST))\n",
    "    results_df.index = ['Ratio of positive samples', 'Model accuracy', 'True positives ratio',\n",
    "                        'True negatives ratio', \"AUC\", \"Recall\", \"Precision\", \"f1-score\", \"Kappa score\"]\n",
    "\n",
    "    # Creating evaluation plots\n",
    "    plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth,\n",
    "                                        os.path.join(output_path, 'TruePositive_vs_allPositives'), LABELS_LIST)\n",
    "    plot_output_coocurances(model_output_rounded, os.path.join(output_path, 'output_coocurances'), LABELS_LIST)\n",
    "    plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth,\n",
    "                                           os.path.join(output_path, 'false_negative_coocurances'), LABELS_LIST)\n",
    "\n",
    "    # Adjusting threshold based on validation set\n",
    "    if (validation_groundtruth is not None and validation_output is not None):\n",
    "        np.savetxt(os.path.join(output_path, 'validation_predictions.out'), validation_output, delimiter=',')\n",
    "        np.savetxt(os.path.join(output_path, 'valid_ground_truth_classes.txt'), validation_groundtruth, delimiter=',')\n",
    "        thresholds = np.arange(0, 1, 0.01)\n",
    "        f1_array = np.zeros((len(LABELS_LIST), len(thresholds)))\n",
    "        for idx, label in enumerate(LABELS_LIST):\n",
    "            f1_array[idx, :] = [\n",
    "                f1_score(validation_groundtruth[:, idx], np.clip(np.round(validation_output[:, idx] - threshold + 0.5), 0, 1))\n",
    "                for threshold in thresholds]\n",
    "        threshold_arg = np.argmax(f1_array, axis=1)\n",
    "        threshold_per_class = thresholds[threshold_arg]\n",
    "\n",
    "        # plot the f1 score across thresholds\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        for idx, x in enumerate(LABELS_LIST):\n",
    "            plt.plot(thresholds, f1_array[idx, :], linewidth=5)\n",
    "        plt.legend(LABELS_LIST, loc='best')\n",
    "        plt.title(\"F1 Score vs different prediction threshold values for each class\")\n",
    "        plt.savefig(os.path.join(output_path, \"f1_score_vs_thresholds.pdf\"), format=\"pdf\")\n",
    "        plt.savefig(os.path.join(output_path, \"f1_score_vs_thresholds.png\"))\n",
    "\n",
    "        # Applying thresholds optimized per class\n",
    "        model_output_rounded = np.zeros_like(model_output)\n",
    "        for idx, label in enumerate(LABELS_LIST):\n",
    "            model_output_rounded[:, idx] = np.clip(np.round(model_output[:, idx] - threshold_per_class[idx] + 0.5), 0, 1)\n",
    "\n",
    "        accuracies_perclass = sum(model_output_rounded == groundtruth) / len(groundtruth)\n",
    "        # Getting the true positive rate perclass\n",
    "        true_positives_ratio_perclass = sum((model_output_rounded == groundtruth) * (groundtruth == 1)) / sum(\n",
    "            groundtruth)\n",
    "        # Get true negative ratio\n",
    "        true_negative_ratio_perclass = sum((model_output_rounded == groundtruth)\n",
    "                                           * (groundtruth == 0)) / (len(groundtruth) - sum(groundtruth))\n",
    "        results_df = results_df.append(\n",
    "            pd.DataFrame([accuracies_perclass, true_positives_ratio_perclass,\n",
    "                          true_negative_ratio_perclass], columns=LABELS_LIST))\n",
    "        # compute additional metrics (AUC,f1,recall,precision)\n",
    "        auc_roc_per_label = roc_auc_score(groundtruth, model_output, average=None)\n",
    "        precision_perlabel = precision_score(groundtruth, model_output_rounded, average=None)\n",
    "        recall_perlabel = recall_score(groundtruth, model_output_rounded, average=None)\n",
    "        f1_perlabel = f1_score(groundtruth, model_output_rounded, average=None)\n",
    "        kappa_perlabel = [cohen_kappa_score(groundtruth[:, x], model_output_rounded[:, x]) for x in\n",
    "                          range(len(LABELS_LIST))]\n",
    "        results_df = results_df.append(\n",
    "            pd.DataFrame([auc_roc_per_label, precision_perlabel, recall_perlabel, f1_perlabel,kappa_perlabel],\n",
    "                         columns=LABELS_LIST))\n",
    "        results_df.index = ['Ratio of positive samples', 'Model accuracy', 'True positives ratio',\n",
    "                            'True negatives ratio', \"AUC\", \"Precision\", \"Recall\", \"f1-score\",  \"Kappa score\",\n",
    "                            'Optimized model accuracy', 'Optimized true positives ratio',\n",
    "                            'Optimized true negatives ratio', \"Optimized AUC\",\n",
    "                            \"Optimized precision\", \"Optimized recall\", \"Optimized f1-score\",  \"Optimized Kappa score\"]\n",
    "\n",
    "        # Creating evaluation plots\n",
    "        plot_true_poisitve_vs_all_positives(model_output_rounded, groundtruth,\n",
    "                                            os.path.join(output_path, 'TruePositive_vs_allPositives[optimized]'),\n",
    "                                            LABELS_LIST)\n",
    "        plot_output_coocurances(model_output_rounded, os.path.join(output_path, 'output_coocurances[optimized]'),\n",
    "                                LABELS_LIST)\n",
    "        plot_false_netgatives_confusion_matrix(model_output_rounded, groundtruth,\n",
    "                                               os.path.join(output_path, 'false_negative_coocurances[optimized]'),\n",
    "                                               LABELS_LIST)\n",
    "    results_df['average'] = results_df.mean(numeric_only=True, axis=1)\n",
    "    results_df.T.to_csv(os.path.join(output_path, \"results_report.csv\"), float_format=\"%.2f\")\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Experiment: user_embeddings_initial\n",
      "\n",
      "\n",
      "\n",
      "INFO:tensorflow:Summary name Original cross_entropy is illegal; using Original_cross_entropy instead.\n",
      "Execute the following in a terminal:\n",
      "tensorboard --logdir=/srv/workspace/research/user_based_contexts_tagging/experiments_results/user_embeddings_initial/2020-04-10_16-19-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'input' with dtype float and shape [?,646,96,1]\n\t [[node input (defined at <ipython-input-4-c6872b9982a8>:9)  = Placeholder[_class=[\"loc:@batch_normalization/cond/FusedBatchNorm_1/Switch\"], dtype=DT_FLOAT, shape=[?,646,96,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node Mean_5/_39}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_887_Mean_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'input', defined at:\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 225, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 708, in __init__\n    self.run()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-c6872b9982a8>\", line 9, in <module>\n    x_input = tf.placeholder(tf.float32, [None, 646, 96, 1], name=\"input\")\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input' with dtype float and shape [?,646,96,1]\n\t [[node input (defined at <ipython-input-4-c6872b9982a8>:9)  = Placeholder[_class=[\"loc:@batch_normalization/cond/FusedBatchNorm_1/Switch\"], dtype=DT_FLOAT, shape=[?,646,96,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node Mean_5/_39}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_887_Mean_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input' with dtype float and shape [?,646,96,1]\n\t [[{{node input}} = Placeholder[_class=[\"loc:@batch_normalization/cond/FusedBatchNorm_1/Switch\"], dtype=DT_FLOAT, shape=[?,646,96,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node Mean_5/_39}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_887_Mean_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-45609eda6a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m             summary, batch_loss[batch_counter], batch_accuracy[batch_counter], _ = sess.run([merged, loss, accuracy, train_step],feed_dict={x_input: batch[0],y:batch_labels,\n\u001b[1;32m     83\u001b[0m                                                         \u001b[0membeddings_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_embeddings\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                                         train_phase: True})\n\u001b[0m\u001b[1;32m     85\u001b[0m         print(\"Epoch #{}\".format(epoch + 1), \"Loss: {:.4f}\".format(np.mean(batch_loss)),\n\u001b[1;32m     86\u001b[0m               \u001b[0;34m\"My_loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_my_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input' with dtype float and shape [?,646,96,1]\n\t [[node input (defined at <ipython-input-4-c6872b9982a8>:9)  = Placeholder[_class=[\"loc:@batch_normalization/cond/FusedBatchNorm_1/Switch\"], dtype=DT_FLOAT, shape=[?,646,96,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node Mean_5/_39}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_887_Mean_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'input', defined at:\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 225, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 708, in __init__\n    self.run()\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-c6872b9982a8>\", line 9, in <module>\n    x_input = tf.placeholder(tf.float32, [None, 646, 96, 1], name=\"input\")\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input' with dtype float and shape [?,646,96,1]\n\t [[node input (defined at <ipython-input-4-c6872b9982a8>:9)  = Placeholder[_class=[\"loc:@batch_normalization/cond/FusedBatchNorm_1/Switch\"], dtype=DT_FLOAT, shape=[?,646,96,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node Mean_5/_39}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_887_Mean_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Experiment: \" + EXPERIMENTNAME + \"\\n\\n\\n\")\n",
    "# Loading datasets\n",
    "#TODO: fix directories\n",
    "training_dataset = get_training_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/train_partial.csv\"))\n",
    "val_dataset = get_validation_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/validation_partial.csv\"))\n",
    "\n",
    "# Setting up model\n",
    "y = tf.placeholder(tf.float32, [None, len(LABELS_LIST)], name=\"true_labels\")\n",
    "x_input = tf.placeholder(tf.float32, [None, 646, 96, 1], name=\"input\")\n",
    "embeddings_input = tf.placeholder(tf.float32, [None, EMBEDDINGS_DIM], name=\"input_embeddings\")\n",
    "weights = tf.constant(POS_WEIGHTS)\n",
    "#current_keep_prob = tf.placeholder(tf.float32, name=\"dropout_rate\")\n",
    "train_phase = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "logits, model_output = get_model(x_input,embeddings_input, 1.0, train_phase)\n",
    "\n",
    "# Defining loss and metrics\n",
    "#loss = tf.reduce_mean(tf.nn.sigmoid_crosws_entropy_with_logits(logits=logits, labels=y))\n",
    "loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=logits, labels=y,pos_weight=POS_WEIGHTS))\n",
    "\n",
    "# Learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.1, global_step=global_step, decay_steps=1000,\n",
    "                                           decay_rate=0.95, staircase=True)\n",
    "'''\n",
    "These following lines are needed for batch normalization to work properly\n",
    "check https://timodenk.com/blog/tensorflow-batch-normalization/\n",
    "'''\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "correct_prediction = tf.equal(tf.round(model_output), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Adding tensorboard summaries\n",
    "tf.summary.scalar('Original cross_entropy', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Setting up dataset iterator\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "training_next_element = training_iterator.get_next()\n",
    "validation_iterator = val_dataset.make_one_shot_iterator()\n",
    "validation_next_element = validation_iterator.get_next()\n",
    "\n",
    "## Setting up early stopping parameters\n",
    "# Best validation accuracy seen so far.\n",
    "best_validation_loss = 10e6  # Just some large number before storing the first validation loss\n",
    "# Iteration-number for last improvement to validation accuracy.\n",
    "last_improvement = 0\n",
    "# Stop optimization if no improvement found in this many iterations.\n",
    "min_epochs_for_early_stop = 10\n",
    "\n",
    "# Training paramaeters\n",
    "#TRAINING_STEPS = 3125\n",
    "#VALIDATION_STEPS = 927\n",
    "TRAINING_STEPS = 2\n",
    "VALIDATION_STEPS = 2\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Setting up saving directory\n",
    "experiment_name = strftime(\"%Y-%m-%d_%H-%M-%S\", localtime())\n",
    "exp_dir = os.path.join(OUTPUT_PATH, EXPERIMENTNAME, experiment_name)\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epoch_losses_history, epoch_accurcies_history, val_losses_history, val_accuracies_history = [], [], [], []\n",
    "my_loss_history, my_loss_val_history = [], []\n",
    "with tf.Session() as sess:\n",
    "    # Write summaries to LOG_DIR -- used by TensorBoard\n",
    "    train_writer = tf.summary.FileWriter(exp_dir + '/tensorboard/train', graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(exp_dir + '/tensorboard/test', graph=tf.get_default_graph())\n",
    "    print(\"Execute the following in a terminal:\\n\" + \"tensorboard --logdir=\" + exp_dir)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        batch_loss, batch_accuracy = np.zeros([TRAINING_STEPS, 1]), np.zeros([TRAINING_STEPS, 1])\n",
    "        batch_my_loss, val_my_loss = np.zeros([TRAINING_STEPS, 1]), np.zeros([VALIDATION_STEPS, 1])\n",
    "        val_accuracies, val_losses = np.zeros([VALIDATION_STEPS, 1]), np.zeros([VALIDATION_STEPS, 1])\n",
    "        for batch_counter in range(TRAINING_STEPS):\n",
    "            batch = sess.run(training_next_element)\n",
    "            batch_labels = np.squeeze(batch[1])\n",
    "            batch_embeddings = np.squeeze(batch[2])\n",
    "            summary, batch_loss[batch_counter], batch_accuracy[batch_counter], _ = sess.run([merged, loss, accuracy, train_step],feed_dict={x_input: batch[0],y:batch_labels,\n",
    "                                                        embeddings_input:batch_embeddings ,\n",
    "                                                        train_phase: True})\n",
    "        print(\"Epoch #{}\".format(epoch + 1), \"Loss: {:.4f}\".format(np.mean(batch_loss)),\n",
    "              \"My_loss: {:.4f}\".format(np.mean(batch_my_loss)),\n",
    "              \"accuracy: {:.4f}\".format(np.mean(batch_accuracy)))\n",
    "        epoch_losses_history.append(np.mean(batch_loss));\n",
    "        epoch_accurcies_history.append(np.mean(batch_accuracy))\n",
    "        my_loss_history.append(np.mean(batch_my_loss))\n",
    "        # Add to summaries\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "        \n",
    "        for validation_batch in range(VALIDATION_STEPS):\n",
    "            val_batch = sess.run(validation_next_element)\n",
    "            summary, val_losses[validation_batch], val_accuracies[validation_batch], = sess.run([merged, loss, accuracy],\n",
    "                                             feed_dict={\n",
    "                                                 x_input: val_batch[0],\n",
    "                                                 y: np.squeeze(val_batch[1]),\n",
    "                                                 embeddings_input: np.squeeze(val_batch[2]),\n",
    "                                                 train_phase: False})\n",
    "        print(\"validation Loss : {:.4f}\".format(np.mean(val_losses)),\n",
    "              \"validation accuracy: {:.4f}\".format(np.mean(val_accuracies)))\n",
    "        val_losses_history.append(np.mean(val_losses));\n",
    "        val_accuracies_history.append(np.mean(val_accuracies))\n",
    "        my_loss_val_history.append(np.mean(val_my_loss))\n",
    "        test_writer.add_summary(summary, epoch)\n",
    "\n",
    "        # If validation loss is an improvement over best-known.\n",
    "        if np.mean(val_my_loss) < best_validation_loss:\n",
    "            # Update the best-known validation accuracy.\n",
    "            best_validation_loss = np.mean(val_my_loss)\n",
    "\n",
    "            # Set the iteration for the last improvement to current.\n",
    "            last_improvement = epoch\n",
    "\n",
    "            # Save all variables of the TensorFlow graph to file.\n",
    "            save_path = saver.save(sess, os.path.join(exp_dir, \"best_validation.ckpt\"))\n",
    "            # print(\"Model with best validation saved in path: %s\" % save_path)\n",
    "\n",
    "        # If no improvement found in the required number of iterations.\n",
    "        if epoch - last_improvement > min_epochs_for_early_stop:\n",
    "            print(\"No improvement found in a last 10 epochs, stopping optimization.\")\n",
    "            # Break out from the for-loop.\n",
    "            break\n",
    "\n",
    "    save_path = saver.save(sess, os.path.join(exp_dir, \"last_epoch.ckpt\"))\n",
    "    print(\"Last iteration model saved in path: %s\" % save_path)\n",
    "\n",
    "    # Loading model with best validation\n",
    "    saver.restore(sess, os.path.join(exp_dir, \"best_validation.ckpt\"))\n",
    "    print(\"Model with best validation restored before testing.\")\n",
    "    \n",
    "    global_labels = pd.read_csv(os.path.join(SOURCE_PATH, \"GroundTruth/test_partial.csv\"))\n",
    "    test_dataset = get_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/test_partial.csv\"))\n",
    "    test_classes = np.zeros_like(global_labels.iloc[:,2:].values, dtype=float)\n",
    "    #test_images, test_classes = load_test_set_raw(test_split)\n",
    "\n",
    "    TEST_NUM_STEPS = int(np.floor((len(global_labels)/32)))\n",
    "    #split_size = int(len(test_classes) / TEST_NUM_STEPS)\n",
    "    test_pred_prob = np.zeros_like(test_classes, dtype=float)\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    test_next_element = test_iterator.get_next()\n",
    "\n",
    "    for test_batch_counter in range(TEST_NUM_STEPS):\n",
    "        start_idx = (test_batch_counter * BATCH_SIZE)\n",
    "        end_idx = (test_batch_counter * BATCH_SIZE) + BATCH_SIZE\n",
    "        test_batch = sess.run(test_next_element)\n",
    "        test_batch_images = test_batch[0]\n",
    "        test_batch_labels = np.squeeze(test_batch[1])\n",
    "        test_embeddings = np.squeeze(test_batch[2])\n",
    "        test_classes[start_idx:end_idx,:] = test_batch_labels\n",
    "        test_pred_prob[start_idx:end_idx,:] = sess.run(model_output,\n",
    "                                                 feed_dict={input_images:test_batch_images,\n",
    "                                                            embeddings_input:test_embeddings,\n",
    "                                                            train_phase:False})\n",
    "\n",
    "    accuracy_out, auc_roc, hamming_error = evaluate_model(test_pred_prob, test_classes,\n",
    "                                                      saving_path=exp_dir,\n",
    "                                                      evaluation_file_path= \\\n",
    "                                                      os.path.join(exp_dir,\"evaluation_results.txt\"))           \n",
    "    results = create_analysis_report(test_pred_prob, test_classes, exp_dir, LABELS_LIST)\n",
    "    \"\"\"\n",
    "    # Running on validation set to adjust the threshold values\n",
    "    # Testing the model [I split the testset into smaller splits because of memory error]\n",
    "    va_spectrograms, val_classes = load_validation_set_raw()\n",
    "    VAL_NUM_STEPS = 607  # number is chosen based on testset size to be dividabable [would change based on dataset]\n",
    "    split_size = int(len(val_classes) / VAL_NUM_STEPS)\n",
    "    val_pred_prob = np.zeros_like(val_classes, dtype=float)\n",
    "    for val_split in range(VAL_NUM_STEPS):\n",
    "        spectrograms_split = va_spectrograms[(val_split * split_size):(val_split * split_size) + split_size, :, :]\n",
    "        val_pred_prob[(val_split * split_size):(val_split * split_size) + split_size, :] = sess.run(model_output,\n",
    "                                                                                                    feed_dict={\n",
    "                                                                                                        x_input: spectrograms_split,\n",
    "                                                                                                        current_keep_prob: 1,\n",
    "                                                                                                        train_phase: False})\n",
    "    \"\"\"\n",
    "# Plot and save losses\n",
    "plot_loss_acuracy(epoch_losses_history, epoch_accurcies_history, val_losses_history, val_accuracies_history,\n",
    "                  exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    global_labels = pd.read_csv(os.path.join(SOURCE_PATH, \"GroundTruth/test_partial.csv\"))\n",
    "    test_dataset = get_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/test_partial.csv\"))\n",
    "    test_classes = np.zeros_like(global_labels.iloc[:,1:].values, dtype=float)\n",
    "    #test_images, test_classes = load_test_set_raw(test_split)\n",
    "\n",
    "    TEST_NUM_STEPS = int(np.floor((len(global_labels)/32)))\n",
    "    #split_size = int(len(test_classes) / TEST_NUM_STEPS)\n",
    "    test_pred_prob = np.zeros_like(test_classes, dtype=float)\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    test_next_element = test_iterator.get_next()\n",
    "    test_batch = sess.run(test_next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    global_labels = pd.read_csv(os.path.join(SOURCE_PATH, \"GroundTruth/test_partial.csv\"))\n",
    "    test_dataset = get_dataset(os.path.join(SOURCE_PATH, \"GroundTruth/test_partial.csv\"))\n",
    "    test_classes = np.zeros_like(global_labels.iloc[:,2:].values, dtype=float)\n",
    "    #test_images, test_classes = load_test_set_raw(test_split)\n",
    "\n",
    "    TEST_NUM_STEPS = int(np.floor((len(global_labels)/32)))\n",
    "    #split_size = int(len(test_classes) / TEST_NUM_STEPS)\n",
    "    test_pred_prob = np.zeros_like(test_classes, dtype=float)\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    test_next_element = test_iterator.get_next()\n",
    "\n",
    "    for test_batch_counter in range(TEST_NUM_STEPS):\n",
    "        start_idx = (test_batch_counter * BATCH_SIZE)\n",
    "        end_idx = (test_batch_counter * BATCH_SIZE) + BATCH_SIZE\n",
    "        test_batch = sess.run(test_next_element)\n",
    "        test_batch_images = test_batch[0]\n",
    "        test_batch_labels = np.squeeze(test_batch[1])\n",
    "        test_embeddings = np.squeeze(test_batch[2])\n",
    "        test_classes[start_idx:end_idx,:] = test_batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
